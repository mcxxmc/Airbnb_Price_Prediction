{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process the data, fill the nan \n",
    "\n",
    "data = pd.read_csv(\"data.csv\", parse_dates=True)\n",
    "\n",
    "\n",
    "# deal with non-numerical values in data\n",
    "\n",
    "# host_since (to years)\n",
    "data[\"host_since\"] = round(((pd.to_datetime(\"9-17-2019\") - pd.to_datetime(data[\"host_since\"])).dt.days/365),1)\n",
    "median = data[\"host_since\"].median()\n",
    "#data[\"host_since\"].replace([np.inf, -np.inf], np.nan)\n",
    "data[\"host_since\"] = data[\"host_since\"].fillna(median)\n",
    "data[\"host_since\"] = pd.to_numeric(data[\"host_since\"])\n",
    "\n",
    "\n",
    "# host_response_time\n",
    "# within an hour - 1\n",
    "# within a few hours - 2\n",
    "# within a day -3\n",
    "# within a few days - 4\n",
    "# within a few days or more - 5\n",
    "data[\"host_response_time\"] = data[\"host_response_time\"].map({'within an hour':1, 'within a few hours':2, 'within a day':3, 'within a few days':4, 'within a few days or more':5})\n",
    "median = data[\"host_response_time\"].median()\n",
    "#data[\"host_response_time\"].replace([np.inf, -np.inf], np.nan)\n",
    "data[\"host_response_time\"] = data[\"host_response_time\"].fillna(median)\n",
    "\n",
    "\n",
    "# host_response_rate\n",
    "data['host_response_rate'] = data['host_response_rate'].str.rstrip('%').astype('float') / (100.0)\n",
    "median = data['host_response_rate'].median()\n",
    "data['host_response_rate'] = data['host_response_rate'].fillna(median)\n",
    "\n",
    "\n",
    "# host_is_superhost\n",
    "data['host_is_superhost'] = data['host_is_superhost'].map({'t':1, 'f':0})\n",
    "median = data['host_is_superhost'].median()\n",
    "data['host_is_superhost'] = data['host_is_superhost'].fillna(median)\n",
    "\n",
    "\n",
    "# host_listings_count\n",
    "median = data['host_listings_count'].median()\n",
    "data['host_listings_count'] = data['host_listings_count'].fillna(median)\n",
    "\n",
    "\n",
    "# host_identity_verified\n",
    "data['host_identity_verified'] = data['host_identity_verified'].map({'t':1, 'f':0})\n",
    "median = data['host_identity_verified'].median()\n",
    "data['host_identity_verified'] = data['host_identity_verified'].fillna(median)\n",
    "\n",
    "\n",
    "# zipcode (standardized by minus). Higher or lower doesn't info price. Should be mapped by average price within each area.\n",
    "data['zipcode'] = pd.to_numeric(data['zipcode'])\n",
    "median = data['zipcode'].median()\n",
    "data['zipcode'] = data['zipcode'].fillna(median) \n",
    "mini = min(data['zipcode'])\n",
    "data['zipcode'] = data['zipcode'] - mini + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# latitude and longitude, overlap with zipcode, so delete\n",
    "data = data.drop(columns = ['latitude', 'longitude'])\n",
    "\n",
    "\n",
    "# is_location_exact\n",
    "data['is_location_exact'] = data['is_location_exact'].map({'t':1, 'f':0})\n",
    "median = data['is_location_exact'].median()\n",
    "data['is_location_exact'] = data['is_location_exact'].fillna(median)\n",
    "\n",
    "\n",
    "# property_type\n",
    "m = lambda x: 1 if x not in ['Apartment', 'House'] else (2 if x=='Apartment' else 3)\n",
    "data['property_type'] = data['property_type'].map(m)\n",
    "median = data['property_type'].median()\n",
    "data['property_type'] = data['property_type'].fillna(median)\n",
    "\n",
    "\n",
    "# room_type\n",
    "m = lambda x: 1 if x not in ['Entire home/apt', 'Private room'] else (2 if x=='Private room' else 3)\n",
    "data['room_type'] = data['room_type'].map(m)\n",
    "median = data['room_type'].median()\n",
    "data['room_type'] = data['room_type'].fillna(median)\n",
    "\n",
    "\n",
    "# accommodates\n",
    "median = data['accommodates'].median()\n",
    "data['accommodates'] = data['accommodates'].fillna(median)\n",
    "\n",
    "\n",
    "\n",
    "# bathrooms\n",
    "median = data['bathrooms'].median()\n",
    "data['bathrooms'] = data['bathrooms'].fillna(median)\n",
    "\n",
    "\n",
    "# bedrooms\n",
    "median = data['bedrooms'].median()\n",
    "data['bedrooms'] = data['bedrooms'].fillna(median)\n",
    "\n",
    "\n",
    "# beds\n",
    "median = data['beds'].median()\n",
    "data['beds'] = data['beds'].fillna(median)\n",
    "\n",
    "\n",
    "# bed_type\n",
    "m = lambda x: 1 if x == 'Real Bed' else 0\n",
    "data['bed_type'] = data['bed_type'].map(m)\n",
    "median = data['bed_type'].median()\n",
    "data['bed_type'] = data['bed_type'].fillna(median)\n",
    "\n",
    "\n",
    "# amenities\n",
    "m = lambda x: x.count(\",\")+1\n",
    "data['amenities'] = data['amenities'].map(m)\n",
    "median = data['amenities'].median()\n",
    "data['amenities'] = data['amenities'].fillna(median)\n",
    "\n",
    "\n",
    "# square_feet. The valid sample size we have is too small, delete\n",
    "data = data.drop(columns = ['square_feet'])\n",
    "\n",
    "\n",
    "# number_of_reviews\n",
    "# number_of_reviews_ltm\n",
    "# may be high due to being cheap, and it varies too much. All reviews may be a result instead of a cause. The total score may be useful.\n",
    "\n",
    "# first_review and last_review (by minus), consider omitting\n",
    "\n",
    "# review_scores_rating\n",
    "\n",
    "# review_scores_accuracy\n",
    "\n",
    "# review_scores_cleanliness\n",
    "\n",
    "# review_scores_checkin\n",
    "\n",
    "# review_scores_communication\n",
    "\n",
    "# review_scores_location\n",
    "\n",
    "# review_scores_value\n",
    "\n",
    "reviews_to_be_dropped = ['number_of_reviews','number_of_reviews_ltm','first_review','last_review','review_scores_accuracy',\\\n",
    "                        'review_scores_cleanliness','review_scores_checkin','review_scores_communication',\\\n",
    "                        'review_scores_location','review_scores_value']\n",
    "data = data.drop(columns = reviews_to_be_dropped)\n",
    "\n",
    "median = data['review_scores_rating'].median()\n",
    "data['review_scores_rating'] = data['review_scores_rating'].fillna(median)\n",
    "\n",
    "\n",
    "data.to_csv(\"adjusted_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(\"adjusted_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "val = pd.read_csv(\"val.csv\")\n",
    "\n",
    "test_index = test['id'].tolist()\n",
    "train_index = train['id'].tolist()\n",
    "val_index = val['id'].tolist()\n",
    "\n",
    "test_data = data[data['id'].isin(test_index)]\n",
    "train_data = data[data['id'].isin(train_index)]\n",
    "val_data = data[data['id'].isin(val_index)]\n",
    "\n",
    "test_data.to_csv(\"test_data.csv\")\n",
    "train_data.to_csv(\"train_data.csv\")\n",
    "val_data.to_csv(\"val_data.csv\")\n",
    "\n",
    "# by now, the data has been roughly processed\n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test = pd.read_csv(\"test_data.csv\")\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "#val = pd.read_csv(\"val_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23731.064531859556\n"
     ]
    }
   ],
   "source": [
    "xt = np.array(train.iloc[:,3:-1]) #(6152, 17)\n",
    "yt = np.array(train.iloc[:,-1:]) #(6152, 1)\n",
    "\n",
    "# add column of 1\n",
    "def add_column(X):\n",
    "    return np.insert(X, 0, 1, axis=1) #(6152,18)\n",
    "\n",
    "# compute h(X, theta)\n",
    "def predict(X, theta):\n",
    "    Xa = add_column(X)\n",
    "    return Xa @ theta\n",
    "\n",
    "# loss\n",
    "def loss(X,y,theta):\n",
    "    return ((predict(X,theta) - y)**2).mean()/2\n",
    "\n",
    "theta_init = np.zeros((18,1),dtype=np.float64)\n",
    "\n",
    "print(loss(xt,yt,theta_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated theta value [ 0.11455544  4.43195368  0.15246756  0.19511724 -5.8014343   0.02921851\n",
      " -0.84567614 -1.10062616  0.52970163  3.45689608  7.63186171 18.42360776\n",
      "  9.21938959 13.65540048  5.84865427  0.66377544 -1.48132326  1.58039741]\n",
      "resulting loss 5842.188368382864\n"
     ]
    }
   ],
   "source": [
    "#np.seterr(invalid='ignore')\n",
    "\n",
    "def loss_gradient(X, y, theta):\n",
    "    Xa = add_column(X)\n",
    "    loss_grad = ((predict(X, theta) - y)*Xa).mean(axis=0)[:, np.newaxis]\n",
    "    return loss_grad\n",
    "\n",
    "def run_gd(loss, loss_gradient, X, y, theta_init, lr=0.00009, n_iter=10000):\n",
    "    theta_current = theta_init.copy()\n",
    "    loss_values = []\n",
    "    theta_values = []\n",
    "    for i in range(n_iter):\n",
    "       \n",
    "        loss_value = loss(X, y, theta_current)\n",
    "        lg = loss_gradient(X, y, theta_current)\n",
    "        theta_current = theta_current - lr*lg\n",
    "       \n",
    "        '''print(i) \n",
    "        print(loss_value)\n",
    "        print(lg.ravel())\n",
    "        print( theta_current.ravel())'''\n",
    "        \n",
    "        lr = (9/(i+100000))\n",
    "        \n",
    "        loss_values.append(loss_value)\n",
    "        theta_values.append(theta_current)\n",
    "    return theta_current, loss_values, theta_values\n",
    "\n",
    "result = run_gd(loss, loss_gradient, xt, yt, theta_init)\n",
    "theta_est, loss_values, theta_values = result\n",
    "print('estimated theta value', theta_est.ravel())\n",
    "print('resulting loss', loss(xt, yt, theta_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5687.718574872866\n"
     ]
    }
   ],
   "source": [
    "# test on validity\n",
    "val = pd.read_csv(\"val_data.csv\")\n",
    "\n",
    "xv = np.array(val.iloc[:,3:-1]) \n",
    "yv = np.array(val.iloc[:,-1:]) \n",
    "\n",
    "vali_loss = loss(xv,yv,theta_est)\n",
    "\n",
    "print(vali_loss)\n",
    "\n",
    "vali_predict = pd.DataFrame(predict(xv, theta_est), columns = ['Prediction'])\n",
    "val_data2 = pd.concat([val, vali_predict], axis=1)\n",
    "\n",
    "val_data2.to_csv(\"val_data2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_data.csv\")\n",
    "xte = np.array(test.iloc[:,3:-1]) \n",
    "yte = np.array(test.iloc[:,-1:]) \n",
    "\n",
    "test_predict = pd.DataFrame(predict(xte, theta_est), columns = ['price'])\n",
    "test_output = pd.concat([test['id'], test_predict], axis = 1)\n",
    "\n",
    "test_output.to_csv(\"my_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
